Namespace(Attention=True, batch_size=16, configfile='ohsumed', coordinate=False, decay_step=20000, device=0, epochs=400, graph_embedding_size=16, iterations=3, lambda_val=0.5, layer_depth=5, layer_width=2, lr=0.001, node_embedding_size=16, noise=0.3, num_gcn_channels=2, num_gcn_layers=4, num_graph_capsules=64, random_vec=False, reg_scale=0.1, seed=0)
device :  cuda:0
{'dataset': 'ohsumed', 'window_size_g': 20, 'window_size': 5, 'save_graph': False, 'retrieve_graph': False, 'embed_type': 'global_pmi', 'pmi_c': 1}
['C18', 'C22', 'C07', 'C14', 'C03', 'C21', 'C16', 'C09', 'C08', 'C19', 'C20', 'C15', 'C12', 'C05', 'C13', 'C02', 'C01', 'C17', 'C04', 'C06', 'C11', 'C10', 'C23']
start adj creation  1
end adj creation  28
start global adj creation  28
end global adj creation  388
start svd  388
end svd  519
total docs :  7400
total edges :  6261912
total_possible_edges :  52068408
total dropped edges :  149950
([1, 12, 1, 10, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 4, 3, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 6, 6, 1, 1, 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 12, 4, 1, 1, 1, 1, 1, 2, 3, 1, 2, 1, 2, 6, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 3, 2, 10, 1, 2, 1, 1, 1, 3, 1, 2, 2, 1], [3.5359154896152485, 7.963325130136948, 5.398375772675411, 1.6505537829280248, 5.041700828736679, 2.6873994539038124, 4.274445676023012, 3.029879474731822, 4.688879454113936, 5.365940496922257, 7.344285921730725, 7.018863521296097, 6.613398413187932, 5.227104052068041, 2.128514393074343, 3.3299840671821603, 4.235912012020769, 3.4215512607076506, 4.84033107697203, 3.2998860360248807, 2.055242191968017, 7.829793737512425, 5.28426246590799, 3.1854028383710524, 4.721849473351506, 3.1035127257752757, 3.5653548450659303, 4.014832444927411, 3.499060397226094, 3.0004804998770407, 2.992520258838652, 7.1366465569524795, 4.912023005428146, 2.894420927916716, 3.100195973149282, 3.8022123914500066, 6.508037897530106, 1.3255055639757793, 2.8208265005168665, 2.7050859871563215, 2.9651480709627336, 4.274445676023012, 5.605170185988092, 3.9456559355738148, 4.636235720628514, 3.4049471056556153, 3.7354491752903245, 3.2258743334259865, 5.8838835884571115, 5.490394671395663, 2.3696764498912306, 4.175894002294516, 8.117475809964205, 3.820190403745415, 4.672793316362312, 6.298317366548036, 3.0924993985972926, 3.993033216584826, 6.353887217702847, 4.198808261817391, 5.57850193890593, 4.809368851368063, 2.753058641711646, 3.228380600402046, 6.475248074707115, 3.769350726966006, 4.453914163834559, 3.546207175651796, 4.524740216403172, 5.53978742672524, 1.323262572381198, 3.716872789717389, 5.660740037142902, 2.8075593075728174, 6.443499376392535, 5.365940496922257, 4.00933792560977, 7.606650186198215, 4.529337925651801, 5.217887396963118, 3.430725636983692, 6.964796300025821, 6.91350300563827, 5.324267800521689, 2.7223342587806294, 5.53978742672524, 4.52016354937576, 3.628839440232066, 5.646555402150946, 4.797247490835717, 2.302847889419609, 6.325716340736151, 4.285217773004923, 5.53978742672524, 2.3357040164463108, 3.3667633186854564, 7.829793737512425, 2.5953484475587993, 2.9273006020358725, 4.932501536771687, 4.809368851368063, 6.1025727894219415, 3.4651040224918206, 4.059910499245402, 2.3750069529181714, 2.2720008904027886, 1.039555904458746, 5.28426246590799, 5.61877583804387, 2.2416091876072706, 5.0809215418899605, 5.997212273764115, 4.208791705801575, 4.688879454113936, 4.529337925651801, 3.34256284938902, 4.128491763399932])
total zero edge graphs :  0
Model(
  (word_embeddings): Embedding(14159, 300, padding_idx=0)
  (attention): Attention(
    (linears): ModuleList(
      (0): Linear(in_features=130, out_features=12, bias=False)
      (1): Linear(in_features=12, out_features=1, bias=False)
    )
  )
  (gcn_layers): ModuleList(
    (0): GCN(
      (linear1): Linear(in_features=300, out_features=32, bias=True)
    )
    (1): GCN(
      (linear1): Linear(in_features=32, out_features=32, bias=True)
    )
    (2): GCN(
      (linear1): Linear(in_features=32, out_features=32, bias=True)
    )
    (3): GCN(
      (linear1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
  (graph_capsule): SecondaryCapsuleLayer()
  (class_capsule): SecondaryCapsuleLayer()
  (reconstruction_layer_1): Linear(in_features=16, out_features=200, bias=True)
  (reconstruction_layer_3): Linear(in_features=200, out_features=14159, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
loss recon 105.46919578313828 margin : 116.99105077981949
Epoch :  1 loss training:  127.53797054290771 Time :  973
accuracy train: 0.264064 val: 0.274627 test: 0.251051
max val : 0.2746268656716418 test : 0.25105119960425426 epoch : 1

loss recon 99.90899294614792 margin : 94.91108679771423
Epoch :  2 loss training:  104.9019860625267 Time :  1951
accuracy train: 0.307743 val: 0.268657 test: 0.290131
max val : 0.2746268656716418 test : 0.25105119960425426 epoch : 1

loss recon 98.30506658554077 margin : 86.91928997635841
Epoch :  3 loss training:  96.74979692697525 Time :  2855
accuracy train: 0.359696 val: 0.370149 test: 0.344793
max val : 0.3701492537313433 test : 0.34479347019539947 epoch : 3

loss recon 97.38378083705902 margin : 84.11068993806839
Epoch :  4 loss training:  93.84906828403473 Time :  3781
accuracy train: 0.417604 val: 0.429851 test: 0.381647
max val : 0.4298507462686567 test : 0.3816472916151373 epoch : 4

loss recon 96.80836552381516 margin : 80.97040517628193
Epoch :  5 loss training:  90.65124180912971 Time :  4767
accuracy train: 0.410986 val: 0.405970 test: 0.380163
max val : 0.4298507462686567 test : 0.3816472916151373 epoch : 4

loss recon 96.39507794380188 margin : 76.91485466063023
Epoch :  6 loss training:  86.55436244606972 Time :  5666
accuracy train: 0.452680 val: 0.438806 test: 0.406629
max val : 0.4388059701492537 test : 0.40662874103388574 epoch : 6

loss recon 96.12153673171997 margin : 73.39610728621483
Epoch :  7 loss training:  83.0082611143589 Time :  6471
accuracy train: 0.500662 val: 0.480597 test: 0.445461
max val : 0.48059701492537316 test : 0.4454612911204551 epoch : 7

loss recon 95.91821157932281 margin : 72.47224867343903
Epoch :  8 loss training:  82.06406977772713 Time :  7282
accuracy train: 0.526803 val: 0.498507 test: 0.463765
max val : 0.49850746268656715 test : 0.46376453128864703 epoch : 8

loss recon 95.79153782129288 margin : 69.46373070776463
Epoch :  9 loss training:  79.04288460314274 Time :  8099
accuracy train: 0.529120 val: 0.540299 test: 0.460054
max val : 0.5402985074626866 test : 0.4600544150383379 epoch : 9

loss recon 95.71832227706909 margin : 66.40547992289066
Epoch :  10 loss training:  75.97731223702431 Time :  8896
accuracy train: 0.555592 val: 0.501493 test: 0.476132
max val : 0.5402985074626866 test : 0.4600544150383379 epoch : 9

loss recon 95.64711195230484 margin : 64.38745933771133
Epoch :  11 loss training:  73.95217068493366 Time :  9722
accuracy train: 0.545665 val: 0.495522 test: 0.460054
max val : 0.5402985074626866 test : 0.4600544150383379 epoch : 9

loss recon 95.60336256027222 margin : 62.44758440554142
Epoch :  12 loss training:  72.00792062282562 Time :  10541
accuracy train: 0.550960 val: 0.531343 test: 0.477368
max val : 0.5402985074626866 test : 0.4600544150383379 epoch : 9

loss recon 95.59678941965103 margin : 60.189696818590164
Epoch :  13 loss training:  69.74937568604946 Time :  11088
accuracy train: 0.605890 val: 0.558209 test: 0.512738
max val : 0.5582089552238806 test : 0.5127380657927282 epoch : 13

loss recon 95.66434437036514 margin : 59.89181999862194
Epoch :  14 loss training:  69.45825427770615 Time :  11594
accuracy train: 0.621112 val: 0.543284 test: 0.531041
max val : 0.5582089552238806 test : 0.5127380657927282 epoch : 13

loss recon 95.73662108182907 margin : 58.12729722261429
Epoch :  15 loss training:  67.70095938444138 Time :  12147
accuracy train: 0.638319 val: 0.591045 test: 0.544892
max val : 0.591044776119403 test : 0.544892406628741 epoch : 15

loss recon 95.79512655735016 margin : 55.792091473937035
Epoch :  16 loss training:  65.37160415947437 Time :  12676
accuracy train: 0.627068 val: 0.552239 test: 0.523621
max val : 0.591044776119403 test : 0.544892406628741 epoch : 15

loss recon 95.84567892551422 margin : 53.41630947589874
Epoch :  17 loss training:  63.00087735056877 Time :  13177
accuracy train: 0.650232 val: 0.573134 test: 0.547613
max val : 0.591044776119403 test : 0.544892406628741 epoch : 15

loss recon 95.86868792772293 margin : 52.47255593538284
Epoch :  18 loss training:  62.05942487716675 Time :  13716
accuracy train: 0.672071 val: 0.573134 test: 0.554539
max val : 0.591044776119403 test : 0.544892406628741 epoch : 15

loss recon 95.88705050945282 margin : 50.42350444197655
Epoch :  19 loss training:  60.012209460139275 Time :  14218
accuracy train: 0.683653 val: 0.582090 test: 0.567400
max val : 0.591044776119403 test : 0.544892406628741 epoch : 15

loss recon 95.89737385511398 margin : 50.04035747051239
Epoch :  20 loss training:  59.630094945430756 Time :  14690
accuracy train: 0.682660 val: 0.561194 test: 0.554291
max val : 0.591044776119403 test : 0.544892406628741 epoch : 15

loss recon 95.89734703302383 margin : 46.90105250477791
Epoch :  21 loss training:  56.490787371993065 Time :  15217
accuracy train: 0.697882 val: 0.579104 test: 0.560475
max val : 0.591044776119403 test : 0.544892406628741 epoch : 15

loss recon 95.89443349838257 margin : 45.382156029343605
Epoch :  22 loss training:  54.97159934043884 Time :  15769
accuracy train: 0.702515 val: 0.588060 test: 0.574573
max val : 0.591044776119403 test : 0.544892406628741 epoch : 15

loss recon 95.84600013494492 margin : 45.134403482079506
Epoch :  23 loss training:  54.719003573060036 Time :  16280
accuracy train: 0.704500 val: 0.579104 test: 0.559733
max val : 0.591044776119403 test : 0.544892406628741 epoch : 15

loss recon 95.8081306219101 margin : 44.08511056005955
Epoch :  24 loss training:  53.665923818945885 Time :  16817
accuracy train: 0.715420 val: 0.576119 test: 0.565174
max val : 0.591044776119403 test : 0.544892406628741 epoch : 15

loss recon 95.83357834815979 margin : 44.03671007603407
Epoch :  25 loss training:  53.62006784975529 Time :  17352
accuracy train: 0.723362 val: 0.614925 test: 0.581252
max val : 0.6149253731343284 test : 0.581251545881771 epoch : 25

loss recon 95.85784012079239 margin : 43.20405964553356
Epoch :  26 loss training:  52.78984369337559 Time :  17815
accuracy train: 0.731635 val: 0.605970 test: 0.572842
max val : 0.6149253731343284 test : 0.581251545881771 epoch : 25

loss recon 95.88267260789871 margin : 41.749681025743484
Epoch :  27 loss training:  51.33794824779034 Time :  18312
accuracy train: 0.720715 val: 0.602985 test: 0.566906
max val : 0.6149253731343284 test : 0.581251545881771 epoch : 25

loss recon 95.90227800607681 margin : 41.505686193704605
Epoch :  28 loss training:  51.09591395407915 Time :  18855
accuracy train: 0.739576 val: 0.623881 test: 0.583972
max val : 0.6238805970149254 test : 0.5839722977986643 epoch : 28

loss recon 95.92109042406082 margin : 39.55346591770649
Epoch :  29 loss training:  49.14557486027479 Time :  19353
accuracy train: 0.754798 val: 0.608955 test: 0.579767
max val : 0.6238805970149254 test : 0.5839722977986643 epoch : 28

loss recon 95.93224632740021 margin : 39.304699800908566
Epoch :  30 loss training:  48.897924564778805 Time :  19834
accuracy train: 0.732958 val: 0.582090 test: 0.557754
max val : 0.6238805970149254 test : 0.5839722977986643 epoch : 28

loss recon 95.95118862390518 margin : 39.84537545964122
Epoch :  31 loss training:  49.4404943138361 Time :  20368
accuracy train: 0.749173 val: 0.605970 test: 0.558496
max val : 0.6238805970149254 test : 0.5839722977986643 epoch : 28

loss recon 95.96735680103302 margin : 44.1504287943244
Epoch :  32 loss training:  53.747164487838745 Time :  20858
accuracy train: 0.741893 val: 0.570149 test: 0.557507
max val : 0.6238805970149254 test : 0.5839722977986643 epoch : 28

loss recon 95.9779844880104 margin : 40.38920299708843
Epoch :  33 loss training:  49.987001448869705 Time :  21289
accuracy train: 0.738253 val: 0.591045 test: 0.555281
max val : 0.6238805970149254 test : 0.5839722977986643 epoch : 28

loss recon 95.99143874645233 margin : 40.67465257644653
Epoch :  34 loss training:  50.273796543478966 Time :  21829
accuracy train: 0.779947 val: 0.620896 test: 0.582736
max val : 0.6238805970149254 test : 0.5839722977986643 epoch : 28

loss recon 96.00268286466599 margin : 35.20411101728678
Epoch :  35 loss training:  44.804379403591156 Time :  22345
accuracy train: 0.790867 val: 0.602985 test: 0.577294
max val : 0.6238805970149254 test : 0.5839722977986643 epoch : 28

loss recon 96.00499224662781 margin : 34.53925660252571
Epoch :  36 loss training:  44.139755971729755 Time :  22828
accuracy train: 0.793845 val: 0.626866 test: 0.597081
max val : 0.6268656716417911 test : 0.5970813752164235 epoch : 36

loss recon 96.00849896669388 margin : 33.904248252511024
Epoch :  37 loss training:  43.505098171532154 Time :  23340
accuracy train: 0.771674 val: 0.600000 test: 0.564185
max val : 0.6268656716417911 test : 0.5970813752164235 epoch : 36

loss recon 95.99116545915604 margin : 35.252772368490696
Epoch :  38 loss training:  44.85188899189234 Time :  23866
accuracy train: 0.777631 val: 0.585075 test: 0.571358
max val : 0.6268656716417911 test : 0.5970813752164235 epoch : 36

loss recon 95.99261116981506 margin : 33.22144205868244
Epoch :  39 loss training:  42.820703350007534 Time :  24335
accuracy train: 0.805427 val: 0.608955 test: 0.587188
max val : 0.6268656716417911 test : 0.5970813752164235 epoch : 36

loss recon 95.98899173736572 margin : 31.3011256121099
Epoch :  40 loss training:  40.90002499520779 Time :  24816
accuracy train: 0.798147 val: 0.611940 test: 0.583230
max val : 0.6268656716417911 test : 0.5970813752164235 epoch : 36

loss recon 96.00846922397614 margin : 31.402420714497566
Epoch :  41 loss training:  41.003267861902714 Time :  25336
accuracy train: 0.820979 val: 0.602985 test: 0.583725
max val : 0.6268656716417911 test : 0.5970813752164235 epoch : 36

loss recon 96.02413552999496 margin : 30.853165723383427
Epoch :  42 loss training:  40.455579467117786 Time :  25792
accuracy train: 0.814361 val: 0.635821 test: 0.588177
max val : 0.6358208955223881 test : 0.5881770962156814 epoch : 42

loss recon 96.04321444034576 margin : 32.007387697696686
Epoch :  43 loss training:  41.611709259450436 Time :  26302
accuracy train: 0.806089 val: 0.611940 test: 0.576799
max val : 0.6358208955223881 test : 0.5881770962156814 epoch : 42

loss recon 96.05356073379517 margin : 30.071857284754515
Epoch :  44 loss training:  39.677213445305824 Time :  26841
accuracy train: 0.814692 val: 0.602985 test: 0.565916
max val : 0.6358208955223881 test : 0.5881770962156814 epoch : 42

loss recon 96.07197350263596 margin : 29.075843323022127
Epoch :  45 loss training:  38.68304083496332 Time :  27303
accuracy train: 0.818332 val: 0.594030 test: 0.564927
max val : 0.6358208955223881 test : 0.5881770962156814 epoch : 42

loss recon 96.07635498046875 margin : 28.10360375419259
Epoch :  46 loss training:  37.71123944967985 Time :  27791
accuracy train: 0.839179 val: 0.617910 test: 0.581252
max val : 0.6358208955223881 test : 0.5881770962156814 epoch : 42

loss recon 96.08924114704132 margin : 27.62901284918189
Epoch :  47 loss training:  37.237937182188034 Time :  28293
accuracy train: 0.843481 val: 0.623881 test: 0.586198
max val : 0.6358208955223881 test : 0.5881770962156814 epoch : 42

loss recon 96.10222095251083 margin : 26.36299179121852
Epoch :  48 loss training:  35.97321413457394 Time :  28747
accuracy train: 0.825943 val: 0.594030 test: 0.576057
max val : 0.6358208955223881 test : 0.5881770962156814 epoch : 42

loss recon 96.10978132486343 margin : 27.219321817159653
Epoch :  49 loss training:  36.83030019700527 Time :  29253
accuracy train: 0.847121 val: 0.626866 test: 0.582488
max val : 0.6358208955223881 test : 0.5881770962156814 epoch : 42

loss recon 96.12052237987518 margin : 26.674968985840678
Epoch :  50 loss training:  36.28702136874199 Time :  29862
accuracy train: 0.839841 val: 0.600000 test: 0.581004
max val : 0.6358208955223881 test : 0.5881770962156814 epoch : 42

slurmstepd: error: *** JOB 24157 ON cl-gpusrv2 CANCELLED AT 2021-02-04T08:30:18 ***
